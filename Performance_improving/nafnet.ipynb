!git clone https://github.com/megvii-research/NAFNet
%cd NAFNet

!pip install -r requirements.txt
!pip install --upgrade --no-cache-dir gdown
!python3 setup.py develop --no_cuda_ext

import gdown
gdown.download('https://drive.google.com/uc?id=14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR', "./experiments/pretrained_models/", quiet=False)

gdown.download('https://drive.google.com/uc?id=1uKwZUgeGfBYLlPKllSuzgGUItlzb40hm', "demo_input/", quiet=False)
gdown.download('https://drive.google.com/uc?id=1ov6UqpIA6GjjJT5SdGeUAJECxka14nGf', "demo_input/", quiet=False)

import torch

from basicsr.models import create_model
from basicsr.utils import img2tensor as _img2tensor, tensor2img, imwrite
from basicsr.utils.options import parse
import numpy as np
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img
def img2tensor(img, bgr2rgb=False, float32=True):
    img = img.astype(np.float32) / 255.
    return _img2tensor(img, bgr2rgb=bgr2rgb, float32=float32)

def display(img1, img2):
  fig = plt.figure(figsize=(25, 10))
  ax1 = fig.add_subplot(1, 2, 1)
  plt.title('Input image', fontsize=16)
  ax1.axis('off')
  ax2 = fig.add_subplot(1, 2, 2)
  plt.title('NAFNet output', fontsize=16)
  ax2.axis('off')
  ax1.imshow(img1)
  ax2.imshow(img2)

def single_image_inference(model, img, save_path):
      model.feed_data(data={'lq': img.unsqueeze(dim=0)})

      if model.opt['val'].get('grids', False):
          model.grids()

      model.test()

      if model.opt['val'].get('grids', False):
          model.grids_inverse()

      visuals = model.get_current_visuals()
      sr_img = tensor2img([visuals['result']])
      imwrite(sr_img, save_path)

opt_path = 'options/test/SIDD/NAFNet-width64.yml'
opt = parse(opt_path, is_train=False)
opt['dist'] = False
NAFNet = create_model(opt)

input_path = 'demo_input/noisy-demo-0.png'
output_path = 'demo_output/noisy-demo-0.png'

img_input = imread(input_path)
inp = img2tensor(img_input)
single_image_inference(NAFNet, inp, output_path)
img_output = imread(output_path)
display(img_input, img_output)

!pip install lpips

import math
import numpy as np
import torch
import lpips
from imageio import imread

def calculate_psnr(img1, img2):
    """
    img1, img2: numpy array, RGB, uint8 또는 float32 [0,255]
    """
    img1 = img1.astype(np.float32)
    img2 = img2.astype(np.float32)
    mse = np.mean((img1 - img2) ** 2)
    if mse == 0:
        return float('inf')
    PIXEL_MAX = 255.0
    psnr = 20 * math.log10(PIXEL_MAX / math.sqrt(mse))
    return psnr
  
  
# -------------------------
# LPIPS 계산 함수
# -------------------------
lpips_model = lpips.LPIPS(net="alex").cuda()  # CPU면 .cuda() 제거!

def calculate_lpips(img1, img2):
    """
    img1, img2: numpy array RGB, uint8 [0,255]
    LPIPS 계산을 위해 torch tensor로 변환
    """
    # [H,W,3] → [1,3,H,W] 로 변환 후 [-1,1] 범위로 정규화
    t1 = torch.tensor(img1).permute(2,0,1).float().unsqueeze(0) / 255.0
    t2 = torch.tensor(img2).permute(2,0,1).float().unsqueeze(0) / 255.0

    t1 = (t1 * 2) - 1
    t2 = (t2 * 2) - 1

    t1 = t1.cuda()
    t2 = t2.cuda()

    with torch.no_grad():
        dist = lpips_model(t1, t2)

    return dist.item()

input_path = 'upload/input/00001.png'
output_path = 'upload/output/flower1.png'
gt_path = 'upload/input/_DSC9070.JPG'

img_input = imread(input_path)
inp = img2tensor(img_input)
single_image_inference(NAFNet, inp, output_path)
img_output = imread(output_path)
display(img_input, img_output)
img_gt = imread(gt_path)

print("GT:", img_gt.shape)
print("Input:", img_input.shape)
print("Output:", img_output.shape)


psnr_value = calculate_psnr(img_gt, img_input)
print("GT vs Input PSNR:", psnr_value, "dB")

psnr_value = calculate_psnr(img_gt, img_output)
print("GT vs Output PSNR:", psnr_value, "dB")


# -------------------------
# LPIPS 계산
# -------------------------
lpips_input = calculate_lpips(img_gt, img_input)
lpips_output = calculate_lpips(img_gt, img_output)

print("GT vs Input LPIPS:", lpips_input)
print("GT vs Output LPIPS:", lpips_output)

import os
from google.colab import files
import shutil

upload_folder = 'upload/input'
result_folder = 'upload/output'

os.makedirs(upload_folder, exist_ok=True)
os.makedirs(result_folder, exist_ok=True)

# upload images
uploaded = files.upload()
for filename in uploaded.keys():
  dst_path = os.path.join(upload_folder, filename)
  print(f'move {filename} to {dst_path}')
  shutil.move(filename, dst_path)

import glob
input_list = sorted(glob.glob(os.path.join(upload_folder, '*')))
for input_path in input_list:
  img_input = imread(input_path)
  inp = img2tensor(img_input)
  output_path = os.path.join(result_folder, os.path.basename(input_path))
  single_image_inference(NAFNet, inp, output_path)

# visualize
input_list = sorted(glob.glob(os.path.join(upload_folder, '*')))
output_list = sorted(glob.glob(os.path.join(result_folder, '*')))
for input_path, output_path in zip(input_list, output_list):
  img_input = imread(input_path)
  img_output = imread(output_path)
  display(img_input, img_output)

# download the result
print(f'Download {result_folder}')
os.system(f'zip -r -j download.zip {result_folder}/*')
files.download("download.zip")



